{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d11cc4-f9d4-49bd-846a-2c73481d1e3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21dca703-e3a2-45c3-9f42-42766b9b7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  #객체 생성\n",
    "MAX_MEMORY= '8g'\n",
    "spark= SparkSession.builder.appName(\"taxi-fare-prediction_2nd\")\\\n",
    "            .config('spark.driver.memory', MAX_MEMORY)\\\n",
    "            .config('spark.executor.memory', MAX_MEMORY)\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6d0aa8-90fe-4784-9fd7-a14e05216974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/learning_spark_data/trips/*.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd=os.getcwd()\n",
    "trip_data_path=os.path.join(cwd, 'learning_spark_data', 'trips', '*.csv') #learning_spark_data/trips 폴더 아래의 모든 .csv 파일 경로를 만듦\n",
    "trip_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1f79e7-37ff-4d2a-98f5-d0529a2c28f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:////home/jovyan/work/learning_spark_data/trips/*.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=f\"file:///{trip_data_path.replace(os.sep,'/')}\"\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72694e6c-8c56-4be6-9afa-ca078247b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_df=spark.read.csv(file_path,inferSchema=True, header=True) \n",
    "trip_df.printSchema()\n",
    "\n",
    "#inferSchema=True: 데이터 타입을 자동으로 추론\n",
    "#header=True: 첫 번째 줄을 컬럼명으로 인식\n",
    "#printSchema(): 읽어온 DataFrame의 컬럼명, 타입 구조를 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5cbc11-9a05-43ca-b403-4295e4dc7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df.createOrReplaceTempView('trips')\n",
    "#임시 테이블 trips에서 필요한 컬럼만 추출 후 필터링\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 4\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432ba684-7163-4c60-a195-6af9bfc7227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|              0|               138|                265|         16.5|          0|     Monday|       70.07|\n",
      "|              1|                68|                264|         1.13|          0|     Monday|       11.16|\n",
      "|              1|               239|                262|         2.68|          0|     Monday|       18.59|\n",
      "|              1|               186|                 91|         12.4|          0|     Monday|        43.8|\n",
      "|              2|               132|                265|          9.7|          0|     Monday|        32.3|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = spark.sql(query)\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7d98f4-9467-490c-b961-b61345580718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test split 8:2, seed=1 \n",
    "train_df,test_df=data_df.randomSplit([0.8,0.2],seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c619c1-c62d-45e7-a2a9-fe3b1cebb98c",
   "metadata": {},
   "source": [
    "## 파이프라인 생성\n",
    "- 전처리 과정을 각 스테이지로 정의해서 쌓는다\n",
    "- 범주형: StringIndexer+원핫인코딩 'pickup_location','dropoff_location_id','day_of_week'\n",
    "- 수치형: StandardScaler : 'passenger_count', 'trip_distance', 'pickup_time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba76971d-d674-44ea-bb95-84bf3c0782a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages=[] #파이프라인용 stages 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7d2974-23e0-49fb-a315-4f32e93258bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_98f964e8cd58,\n",
       " OneHotEncoder_df638b420824,\n",
       " StringIndexer_a1196aee1671,\n",
       " OneHotEncoder_aab865d287b6,\n",
       " StringIndexer_97db36c96319,\n",
       " OneHotEncoder_cd309e3acd7f]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "cat_features = ['pickup_location_id', 'dropoff_location_id', 'day_of_week'] #모델에 바로 못넣는 컬럼\n",
    "for cat in cat_features:\n",
    "    cat_index = StringIndexer(inputCol=cat, outputCol=cat+'_idx').setHandleInvalid('keep') #stringindexer로 숫자 인코딩\n",
    "    onehot_encode = OneHotEncoder(inputCols= [cat_index.getOutputCol()], #_idx col    원한인코더로 벡터화(ML용)\n",
    "                                  outputCols=[cat+'_onehot'] #postfix\n",
    "                                 )\n",
    "    stages += [cat_index, onehot_encode ] #collist\n",
    "stages\n",
    "\n",
    "#setHandleInvalid('keep')\n",
    "#  →만약 데이터에 train/test split 등으로 못 본 카테고리가 들어와도\n",
    "#에러 없이 처리(새 인덱스 할당)\n",
    "\n",
    "#처음에 stringindexer로 문자에서 숫자로 변환해주고\n",
    "# 변환된 숫자만 있으면 0,1,2등으로만 보면 순서(서열)가 헷갈려 각 카테고리별로 '독립된 열'을 만들어 희소벡터로 변환함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba149ec2-675c-4ed5-8968-2fb971c8868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_98f964e8cd58,\n",
       " OneHotEncoder_df638b420824,\n",
       " StringIndexer_a1196aee1671,\n",
       " OneHotEncoder_aab865d287b6,\n",
       " StringIndexer_97db36c96319,\n",
       " OneHotEncoder_cd309e3acd7f,\n",
       " VectorAssembler_4024ce8af10d,\n",
       " StandardScaler_7a67f0eb5a13,\n",
       " VectorAssembler_88998bfdd304,\n",
       " StandardScaler_ab264c9e7bd2,\n",
       " VectorAssembler_a5f9de43c1f0,\n",
       " StandardScaler_6f281e523d36]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "num_features= ['passenger_count','trip_distance', 'pickup_time'] #정규화(스케일링)할 수치형 변수 리스트\n",
    "\n",
    "for num in num_features:\n",
    "    num_assembler=VectorAssembler(inputCols=[num],outputCol=num+'_vector') #1차원 벡터컬럼\n",
    "    num_scaler=StandardScaler(inputCol=num_assembler.getOutputCol(),outputCol=num+'_scaled') #위 벡터 데이터 정규화\n",
    "    stages+=[num_assembler, num_scaler] #파이프라인에 추가\n",
    "\n",
    "stages\n",
    "\n",
    "#pyspark의 standardScaler는 반드시 벡터컬럼으로 입력받아야함\n",
    "#예: 3->[3.0]\n",
    "#전처리된 모든 입력을 feature 벡터로 통합\n",
    "\n",
    "'''\n",
    "<통합 전 데이터 프레임>\n",
    "trip_distance\tpassenger_count\tpickup_location_id_onehot\tday_of_week_onehot\n",
    "3.0\t2\t[0, 0, 1, 0, ...]\t[1, 0, 0, 0, 0, 0, 0]\n",
    "5.2\t1\t[0, 1, 0, 0, ...]\t[0, 1, 0, 0, 0, 0, 0]\n",
    "'''\n",
    "'''\n",
    "<통합 후 프레임>\n",
    "features\n",
    "[3.0, 2, 0, 0, 1, 0, ..., 1, 0, 0, 0, 0, 0, 0]\n",
    "[5.2, 1, 0, 1, 0, 0, ..., 0, 1, 0, 0, 0, 0, 0]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026770d1-064f-485e-baec-90ce01a92be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_scaled',\n",
       " 'trip_distance_scaled',\n",
       " 'pickup_time_scaled']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_input = [cat+'_onehot' for cat in cat_features] + [num+'_scaled' for num in num_features]\n",
    "assembler_input\n",
    "\n",
    "#cat_features 원핫인코딩 결과 컬럼과 num_features 스케일링 결과 컬럼을 한번에 묶어서 합친 컬럼명 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "491c9461-b53c-428c-9d14-3f032e33ede8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_98f964e8cd58,\n",
       " OneHotEncoder_df638b420824,\n",
       " StringIndexer_a1196aee1671,\n",
       " OneHotEncoder_aab865d287b6,\n",
       " StringIndexer_97db36c96319,\n",
       " OneHotEncoder_cd309e3acd7f,\n",
       " VectorAssembler_4024ce8af10d,\n",
       " StandardScaler_7a67f0eb5a13,\n",
       " VectorAssembler_88998bfdd304,\n",
       " StandardScaler_ab264c9e7bd2,\n",
       " VectorAssembler_a5f9de43c1f0,\n",
       " StandardScaler_6f281e523d36,\n",
       " VectorAssembler_e9b042512570]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler=VectorAssembler(inputCols=assembler_input, outputCol='feature_vector')\n",
    "stages+=[assembler]\n",
    "stages\n",
    "\n",
    "#위에서 만든 모든 피처(벡터,스케일링,원핫인코딩)들을 하나의 벡터 컬럼으로 합쳐서 stages 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c12fcd-a773-4c8b-9e80-0d3a6769076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline=Pipeline(stages=stages) #파이프라인 객체에 지금까지 쌓은 모든 전처리 단계를 넣기\n",
    "fitted_transform=pipeline.fit(train_df) #한번에 학습, 학습된(인코딩 등) 파이프라인 모델\n",
    "vtrain_df=fitted_transform.transform(train_df) #학습된 모든 전처리 적용\n",
    "vtrain_df.printSchema() #모든 파생 피처 컬럼과 최종feature_vector가 포함된 완전 전처리 데이터프레임\n",
    "\n",
    "#vtrain_df는 전처리 및 피처 엔지니어링이 끝난 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab0bce6-2116-4d52-8e2f-ad7667fdaa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      feature_vector|\n",
      "+--------------------+\n",
      "|(533,[62,311,527,...|\n",
      "|(533,[62,280,526,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_df.select('feature_vector').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "560d9687-c143-48da-a635-f99f0bf873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=50, solver='normal',  \n",
    "                 labelCol='total_amount', featuresCol='feature_vector')\n",
    "\n",
    "#labelCol='total_amount': 예측하고자 하는 값(요금 컬럼) 지정\n",
    "#featuresCol='feature_vector': 입력값으로 쓸 벡터 컬럼명 지정 (여기서는 feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61f73089-38a0-4682-8e7a-147802e1490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(vtrain_df)\n",
    "\n",
    "#학습 데이터(vtrain_df)에서 feature_vector → total_amount를 예측하도록 모델 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09bff106-1ba5-4f51-95f3-cf7337fe239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트데이터(test_df)에도 전처리 변환\n",
    "vtest_df = fitted_transform.transform(test_df)\n",
    "#테스트데이터로 total_amount(요금) 예측\n",
    "pred = model.transform(vtest_df)\n",
    "#pred DataFrame에는 원본 데이터 + 예측값(prediction) 컬럼이 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c2777b-59e7-4ce3-9811-0888ba22d6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|total_amount|        prediction|\n",
      "+------------+------------------+\n",
      "|       10.55|12.695522792729275|\n",
      "|        13.3|14.450558014776915|\n",
      "|        21.3|21.108271361254218|\n",
      "+------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('total_amount', 'prediction').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903462e2-b9e7-4007-856c-88ae4df0bfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80849012500813, 5.6485201652667625)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2, model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e68ed-a33c-4c75-9de2-6a591dd6d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
